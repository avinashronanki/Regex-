{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "REGEX & tokenization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN+RQGjdD4LXlCIuSCKbnw/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avinashronanki/Regex-/blob/master/REGEX_%26_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrP2Uew3OXoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/\n",
        "#http://www.pyregex.com/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNDaSM_M9KBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvk1Ws2AEHF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucq40MIoCOxD",
        "colab_type": "code",
        "outputId": "cf82fa31-1573-419c-e8c9-dc44f8c108bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Find all capitalized words in my_string and print the result\n",
        "capitalized_words = r\"Let's write RegEx!\"\n",
        "print(re.findall(\"[A-Z]\\w+\",capitalized_words))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Let', 'RegEx']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mic2gvZGCmsC",
        "colab_type": "code",
        "outputId": "61185713-719e-4c77-ccfb-a340484c3162",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# Write a pattern to match sentence endings: sentence_endings\n",
        "sentence_endings = r\"[.?!]\"\n",
        "\n",
        "# Split my_string on sentence endings and print the result\n",
        "print(re.split(sentence_endings,my_string))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx2nbqRhEbmW",
        "colab_type": "code",
        "outputId": "9e431238-2ff8-46d5-cec5-d44112d9a589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Split my_string on spaces and print the result\n",
        "spaces = r\"\\s+\"\n",
        "print(re.split(spaces,my_string))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-afvcmIE6hW",
        "colab_type": "code",
        "outputId": "dc5baea7-05c8-4572-8fa7-dbbae7ec557c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Find all digits in my_string and print the result\n",
        "digits = r\"\\d+\"\n",
        "print(re.findall(digits,my_string))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['4', '1', '9']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHgjmDvUWF8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Tweets = ['This is the best #nlp exercise ive found online! #python', '#NLP is super fun! <3 #learning', 'Thanks @datacamp :) #nlp #python']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK3LxMGLSMJz",
        "colab_type": "code",
        "outputId": "9ca916d2-1367-4b5c-c417-bc68784a2dae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Import the necessary modules\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "# Define a regex pattern to find hashtags: pattern1\n",
        "pattern1 = r\"#\\w+\"\n",
        "# Use the pattern on the first tweet in the tweets list\n",
        "hashtags = regexp_tokenize(Tweets[2],pattern1)\n",
        "print(hashtags)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#nlp', '#python']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzIuIFrxZYiN",
        "colab_type": "code",
        "outputId": "a82553fc-b159-4055-9f15-7cbc980c5a4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Import the necessary modules\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "# Use the TweetTokenizer to tokenize all tweets into one list\n",
        "tknzr = TweetTokenizer()\n",
        "all_tokens = [tknzr.tokenize(t) for t in Tweets]\n",
        "print(all_tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKL5YuwNYJKP",
        "colab_type": "code",
        "outputId": "b7b649af-13fe-45d3-975b-406be1a4e58b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Import the necessary modules\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "# Write a pattern that matches both mentions (@) and hashtags\n",
        "pattern2 = r\"([@#]\\w+)\"\n",
        "# Use the pattern on the last tweet in the tweets list\n",
        "mentions_hashtags = regexp_tokenize(Tweets[-1],pattern2)\n",
        "print(mentions_hashtags)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['@datacamp', '#nlp', '#python']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW_JT8JzD_m3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenization using Python’s split() function\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "# Splits at space \n",
        "text.split() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Schlfdg4M0rW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Sentence Tokenization using Split fuction \n",
        "text.split('. ') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA8k6WtVNaG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#One major drawback of using Python’s split() method is that we can use only one separator at a time.\n",
        "# Another thing to note – in word tokenization, split() did not consider punctuation as a separate token.\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1Sla6E0NlMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenization using Regular Expressions (RegEx)\n",
        "import re\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "tokens = re.findall(\"[\\w']+\", text)\n",
        "tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhacLq9nOD6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Sentence Tokenization using Split fuction using regex\n",
        "import re\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
        "species by building a self-sustaining city on, Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "sentences = re.compile('[.!?] ').split(text)\n",
        "sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxJ8XgPcRps6",
        "colab_type": "code",
        "outputId": "b5c22810-32d2-4f6a-b6ac-46fc601530ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Tokenization using NLTK\n",
        "pip install nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AZbca0rSKYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#word_tokenize using nltk\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize \n",
        "nltk.download('punkt')\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "word_tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75cO1q2bSvZ-",
        "colab_type": "code",
        "outputId": "d2610a53-59c3-4195-9530-48a71a638a60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#Sentence Tokenization using nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "sent_tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \\nspecies by building a self-sustaining city on Mars.',\n",
              " 'In 2008, SpaceX’s Falcon 1 became the first privately developed \\nliquid-fuel launch vehicle to orbit the Earth.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2hjrv14R4El",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenization using the spaCy library\n",
        "pip install -U spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HajiGE7AXCJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "\n",
        "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "my_doc = nlp(text)\n",
        "\n",
        "# Create list of word tokens\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "    token_list.append(token.text)\n",
        "token_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fn1BacSXFv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(token_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT5mds-rXq8L",
        "colab_type": "code",
        "outputId": "e2d03aad-97a6-4c75-ac7b-d773c16248f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#Sentence Tokenization\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "# Create the pipeline 'sentencizer' component\n",
        "sbd = nlp.create_pipe('sentencizer')\n",
        "\n",
        "# Add the component to the pipeline\n",
        "nlp.add_pipe(sbd)\n",
        "\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "\n",
        "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "doc = nlp(text)\n",
        "\n",
        "# create list of sentence tokens\n",
        "sents_list = []\n",
        "for sent in doc.sents:\n",
        "    sents_list.append(sent.text)\n",
        "sents_list"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \\nspecies by building a self-sustaining city on Mars.',\n",
              " 'In 2008, SpaceX’s Falcon 1 became the first privately developed \\nliquid-fuel launch vehicle to orbit the Earth.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13CfV9ZFX7G8",
        "colab_type": "code",
        "outputId": "731365a1-0088-4918-a94f-eb96db1c5e25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#Tokenization using Keras\n",
        "pip install Keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras) (1.18.1)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras) (2.8.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ihndX2IZoeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Word Tokenization\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "# define\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "# tokenize\n",
        "result = text_to_word_sequence(text)\n",
        "result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYd9FTGDcWgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenization using Gensim\n",
        "pip install gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0SfIV47eXmL",
        "colab_type": "code",
        "outputId": "55f815c6-c117-438a-dc6e-17a7a3d39618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from gensim.summarization.textcleaner import split_sentences\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "result = split_sentences(text)\n",
        "result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet ',\n",
              " 'species by building a self-sustaining city on Mars.',\n",
              " 'In 2008, SpaceX’s Falcon 1 became the first privately developed ',\n",
              " 'liquid-fuel launch vehicle to orbit the Earth.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6pSdjGueqcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.utils import tokenize\n",
        "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
        "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
        "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
        "list(tokenize(text))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}